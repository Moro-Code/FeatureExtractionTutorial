{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:center\">\n",
    "<h1> Feature Extraction Tutorial </h1>\n",
    "<h3> Keyword extraction has six main seteps </h3>\n",
    "<img src=\"./steps.jpeg\" width = \"600px\" height= \"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> We first need to import the needed packages </h2>\n",
    "<p>This includes: </p>\n",
    "<ol>\n",
    "    <li> <b>nltk.corpus.stopwords</b> : This package allows us to get curated stopwords for many different languages </li>\n",
    "    <li> <b> nltk.stem.wordnet.WordNetLemmatizer </b>: This package is used for the lemmatization process. The WordNetLemmatizer uses the Princeton University WordNet database to get the root of words. WordNet has many other uses as well. More information \n",
    "        <a href=\"https://pythonprogramming.net/wordnet-nltk-tutorial/\"> here </a> </li>\n",
    "    <li> <b> re </b>: a regular expression package for python and my prefered choice for text extraction, and cleaning. NLTK also has really utilities for text cleaning as well. More information on what regular expressions are can be found <a href = \"https://www.regular-expressions.info/\" > here </a></li>\n",
    "    <li> <b> sklearn.feature_extraction.text.CountVectorizer </b>: This package is used to generate the frequency matrix. More information can be found \n",
    " <a href = \"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">here </a> </li>\n",
    "    <li> <b>pandas</b>: Since we'll be using a csv of comments today, one of the best way to manipulate this data is by using a pandas dataframe constructed from this csv. An awesome tutorial on pandas can be found <a href = \"https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python\"></a></li>\n",
    "    \n",
    "</ol>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If this is the first time importing the stopwords package /\n",
    "#you must first download the stopwords using nltk.download('stopwords')\n",
    "#nltk.download('stopwords')\n",
    "import re\n",
    "import pandas\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importing our data </h2>\n",
    "<p> I have provided a very popular dataset called the NIPS papers you can find more information about this dataset <a href= \"https://www.kaggle.com/benhamner/nips-papers/home\"> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 7241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we construct a dataframe from the papers.csv in the nips-paper folder\n",
    "nips_papers = pandas.read_csv(\"nips-papers/papers.csv\")\n",
    "#lets look at some of the data\n",
    "print(\"Number of rows : \" + str(nips_papers.shape[0]))\n",
    "nips_papers.head()\n",
    "#Its clear we need to do some cleaning, we want only rows which have abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows : 3924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>1861</td>\n",
       "      <td>2000</td>\n",
       "      <td>Algorithms for Non-negative Matrix Factorization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1861-algorithms-for-non-negative-matrix-factor...</td>\n",
       "      <td>Non-negative matrix factorization (NMF) has pr...</td>\n",
       "      <td>Algorithms for Non-negative Matrix\\nFactorizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1975</td>\n",
       "      <td>2001</td>\n",
       "      <td>Characterizing Neural Gain Control using Spike...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1975-characterizing-neural-gain-control-using-...</td>\n",
       "      <td>Spike-triggered averaging techniques are effec...</td>\n",
       "      <td>Characterizing neural gain control using\\nspik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>3163</td>\n",
       "      <td>2007</td>\n",
       "      <td>Competition Adds Complexity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3163-competition-adds-complexity.pdf</td>\n",
       "      <td>It is known that determinining whether a DEC-P...</td>\n",
       "      <td>Competition adds complexity\\n\\nJudy Goldsmith\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>3164</td>\n",
       "      <td>2007</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3164-efficient-principled-learning-of-thin-jun...</td>\n",
       "      <td>We present the first truly polynomial algorith...</td>\n",
       "      <td>Efficient Principled Learning of Thin Junction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>3167</td>\n",
       "      <td>2007</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3167-regularized-boost-for-semi-supervised-lea...</td>\n",
       "      <td>Semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>Regularized Boost for Semi-Supervised Learning...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  year                                              title  \\\n",
       "941   1861  2000   Algorithms for Non-negative Matrix Factorization   \n",
       "1067  1975  2001  Characterizing Neural Gain Control using Spike...   \n",
       "2384  3163  2007                        Competition Adds Complexity   \n",
       "2385  3164  2007  Efficient Principled Learning of Thin Junction...   \n",
       "2388  3167  2007     Regularized Boost for Semi-Supervised Learning   \n",
       "\n",
       "     event_type                                           pdf_name  \\\n",
       "941         NaN  1861-algorithms-for-non-negative-matrix-factor...   \n",
       "1067        NaN  1975-characterizing-neural-gain-control-using-...   \n",
       "2384        NaN               3163-competition-adds-complexity.pdf   \n",
       "2385        NaN  3164-efficient-principled-learning-of-thin-jun...   \n",
       "2388        NaN  3167-regularized-boost-for-semi-supervised-lea...   \n",
       "\n",
       "                                               abstract  \\\n",
       "941   Non-negative matrix factorization (NMF) has pr...   \n",
       "1067  Spike-triggered averaging techniques are effec...   \n",
       "2384  It is known that determinining whether a DEC-P...   \n",
       "2385  We present the first truly polynomial algorith...   \n",
       "2388  Semi-supervised inductive learning concerns ho...   \n",
       "\n",
       "                                             paper_text  \n",
       "941   Algorithms for Non-negative Matrix\\nFactorizat...  \n",
       "1067  Characterizing neural gain control using\\nspik...  \n",
       "2384  Competition adds complexity\\n\\nJudy Goldsmith\\...  \n",
       "2385  Efficient Principled Learning of Thin Junction...  \n",
       "2388  Regularized Boost for Semi-Supervised Learning...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we select the rows only when the column \"abstract\" is not equal to the default value \n",
    "nips_papers = nips_papers[nips_papers['abstract'] != \"Abstract Missing\"]\n",
    "print(\"Number of rows : \" + str(nips_papers.shape[0]))\n",
    "nips_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum \t582286\n",
      "count    3924.000000\n",
      "mean      148.390928\n",
      "std        45.605755\n",
      "min        19.000000\n",
      "25%       116.000000\n",
      "50%       143.000000\n",
      "75%       177.000000\n",
      "max       317.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>abstract</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>941</td>\n",
       "      <td>Non-negative matrix factorization (NMF) has pr...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1067</td>\n",
       "      <td>Spike-triggered averaging techniques are effec...</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2384</td>\n",
       "      <td>It is known that determinining whether a DEC-P...</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2385</td>\n",
       "      <td>We present the first truly polynomial algorith...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2388</td>\n",
       "      <td>Semi-supervised inductive learning concerns ho...</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           abstract  word_count\n",
       "0    941  Non-negative matrix factorization (NMF) has pr...         107\n",
       "1   1067  Spike-triggered averaging techniques are effec...          81\n",
       "2   2384  It is known that determinining whether a DEC-P...          67\n",
       "3   2385  We present the first truly polynomial algorith...         143\n",
       "4   2388  Semi-supervised inductive learning concerns ho...         119"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets remove all but the data we want\n",
    "abstract_dataset = pandas.DataFrame(columns=['abstract', 'word_count'])\n",
    "abstract_dataset['abstract'] = nips_papers['abstract']\n",
    "abstract_dataset.reset_index(inplace = True)\n",
    "abstract_dataset['word_count'] = abstract_dataset['abstract'].apply(lambda x: len(x.split(\" \")))\n",
    "total_wordcount = abstract_dataset['word_count'].sum()\n",
    "print(\"sum \\t\" + str(total_wordcount))\n",
    "print(abstract_dataset.word_count.describe())\n",
    "abstract_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./toomuch.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Looking at our data</h2>\n",
    "<p> lets look at the most common words without cleaning our data </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_raw_word_counts = pandas.Series(''.join(abstract_dataset['abstract']).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the                      29793\n",
       "of                       20918\n",
       "a                        16339\n",
       "and                      13626\n",
       "to                       12869\n",
       "in                        8980\n",
       "that                      7838\n",
       "is                        7666\n",
       "for                       7169\n",
       "on                        5579\n",
       "we                        5167\n",
       "We                        4995\n",
       "with                      4512\n",
       "this                      3677\n",
       "as                        3643\n",
       "are                       3529\n",
       "an                        3366\n",
       "by                        3197\n",
       "can                       2953\n",
       "learning                  2825\n",
       "which                     2781\n",
       "be                        2673\n",
       "from                      2622\n",
       "our                       2446\n",
       "model                     2408\n",
       "algorithm                 2262\n",
       "show                      2214\n",
       "In                        2203\n",
       "data                      2143\n",
       "The                       2109\n",
       "                         ...  \n",
       "influenced.                  1\n",
       "20-layer                     1\n",
       "quarter                      1\n",
       "MC-SSDA                      1\n",
       "HMDB                         1\n",
       "problems.Despite             1\n",
       "8\\%                          1\n",
       "massive-dimensional.         1\n",
       "harmonically-related         1\n",
       "make,                        1\n",
       "IPDFs                        1\n",
       "methods.Fine-grained         1\n",
       "unethical).                  1\n",
       "plausible,                   1\n",
       "ExtremeHunter                1\n",
       "planning.Despite             1\n",
       "Revenge'.We                  1\n",
       "paths)                       1\n",
       "O(T^{2/3}                    1\n",
       "facilitatory                 1\n",
       "deformations;                1\n",
       "negatives,                   1\n",
       "order-$K$                    1\n",
       "rival                        1\n",
       "heights                      1\n",
       "weak-oracle.Mirror           1\n",
       "fails.                       1\n",
       "efficiencies.                1\n",
       "root-tree                    1\n",
       "practice.Furthermore,        1\n",
       "Length: 37676, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can see we can't really get useful informative words the way that the data currently is\n",
    "abstract_raw_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Cleaning our data </h2>\n",
    "<p> To clean our data we need to do the following </p>\n",
    "<ul>\n",
    "    <li> remove punctuation and special symbols </li>\n",
    "    <li> remove tags </li>\n",
    "    <li> remove digits </li>\n",
    "    <li> make all characters lower case </li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Non negative matrix factorization NMF has previously been shown to be a useful decomposition for multivariate data Two different multi plicative algorithms for NMF are analyzed They differ only slightly in the multiplicative factor used in the update rules One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback Leibler divergence The monotonic convergence of both algorithms can be proven using an auxiliary func tion analogous to that used for proving convergence of the Expectation Maximization algorithm The algorithms can also be interpreted as diag onally rescaled gradient descent where the rescaling factor is optimally chosen to ensure convergence  Spike triggered averaging techniques are effective for linear characterization of neural responses But neurons exhibit important nonlinear behaviors such as gain control that are not captured by such analyses We describe a spike triggered covariance method for retrievi'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we loop through all our rows\n",
    "corpus = \"\"\n",
    "for abstract in abstract_dataset['abstract']:\n",
    "    #remove all punctuation\n",
    "    abstract = re.sub('[^a-zA-Z]', ' ', abstract)\n",
    "    #empty tags \n",
    "    abstract = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \", abstract)\n",
    "    #remove special characters and digits\n",
    "    abstract = re.sub(\"(\\\\d|\\\\W)+\",\" \", abstract)\n",
    "    #append to corpus\n",
    "    corpus = \" \".join((corpus, abstract))\n",
    "corpus[:1000]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Normalization and Lemmatization </h2>\n",
    "<p> Here we remove all stopwords and get the lemmas for our words</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Non negative matrix factorization NMF previously shown useful decomposition multivariate data Two different multi plicative algorithm NMF analyzed They differ slightly multiplicative factor used update rule One algorithm shown minimize conventional least square error minimizes generalized Kullback Leibler divergence The monotonic convergence algorithm proven using auxiliary func tion analogous used proving convergence Expectation Maximization algorithm The algorithm also interpreted diag onally rescaled gradient descent rescaling factor optimally chosen ensure convergence',\n",
       " 'Spike triggered averaging technique effective linear characterization neural response But neuron exhibit important nonlinear behavior gain control captured analysis We describe spike triggered covariance method retrieving suppressive component gain control signal neuron We demonstrate method simulation retinal ganglion cell data Analysis physiological data reveals significant suppressive ax explains neural nonlinearities This method applicable sensory area modality']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "#initialize the lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "#create a set of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for abstract in abstract_dataset['abstract']:\n",
    "    #remove all punctuation\n",
    "    abstract = re.sub('[^a-zA-Z]', ' ', abstract)\n",
    "    #empty tags \n",
    "    abstract = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \", abstract)\n",
    "    #remove special characters and digits\n",
    "    abstract = re.sub(\"(\\\\d|\\\\W)+\",\" \", abstract)\n",
    "    #make all chars lowercase\n",
    "    abstract.lower()\n",
    "    #create a list of words \n",
    "    words = abstract.split()\n",
    "    text = \" \".join([lem.lemmatize(word) for word in words if \\\n",
    "                    word not in stop_words])\n",
    "    corpus.append(text)\n",
    "# we have now generated a normalized corpus with only word lemmas and \n",
    "corpus[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create the count vector </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words= stop_words, max_features = 10000, ngram_range= (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3924x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 265145 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the corpus into the CountVectorizer\n",
    "cv = cv.fit(corpus)\n",
    "frequency_matrix = cv.transform(corpus)\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = frequency_matrix.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = [(word, sum_words[0,idx]) for word, idx in cv.vocabulary_.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 5023),\n",
       " ('algorithm', 4364),\n",
       " ('learning', 3604),\n",
       " ('method', 3328),\n",
       " ('problem', 3317),\n",
       " ('data', 3150),\n",
       " ('show', 2418),\n",
       " ('approach', 2006),\n",
       " ('function', 1953),\n",
       " ('based', 1854),\n",
       " ('network', 1785),\n",
       " ('result', 1741),\n",
       " ('time', 1486),\n",
       " ('paper', 1459),\n",
       " ('using', 1455),\n",
       " ('task', 1402),\n",
       " ('distribution', 1352),\n",
       " ('propose', 1351),\n",
       " ('state', 1277),\n",
       " ('feature', 1248)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequencies = sorted(word_frequencies, key = lambda x: x[1], reverse = True)\n",
    "word_frequencies[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
